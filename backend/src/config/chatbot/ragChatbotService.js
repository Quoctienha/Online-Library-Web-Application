import ollama from 'ollama';
import Book from '../../models/Book.js';
import { generateEmbedding } from './embedding.js';
import { searchBooks } from './bookSearchService.js';

const CHAT_MODEL = process.env.OLLAMA_CHAT_MODEL || 'llama2';
const DEFAULT_TOP_K = 3;


/**
 * T·∫°o context t·ª´ c√°c s√°ch t√¨m ƒë∆∞·ª£c
 */
function buildContext(books) {
  if (!books || books.length === 0) {
    return 'Kh√¥ng t√¨m th·∫•y t√†i li·ªáu li√™n quan trong c∆° s·ªü d·ªØ li·ªáu.';
  }

  let context = 'D∆∞·ªõi ƒë√¢y l√† c√°c t√†i li·ªáu li√™n quan:\n\n';

  books.forEach((book, index) => {
    context += `[T√†i li·ªáu ${index + 1}]\n`;
    context += `Ti√™u ƒë·ªÅ: ${book.title}\n`;
    context += `T√°c gi·∫£: ${book.author}\n`;
    context += `Th·ªÉ lo·∫°i: ${book.category}\n`;
    context += `M√¥ t·∫£: ${book.description}\n`;
    if (book.publishYear) context += `NƒÉm xu·∫•t b·∫£n: ${book.publishYear}\n`;
    if (book.score) context += `ƒê·ªô li√™n quan: ${(book.score * 100).toFixed(1)}%\n`;
    context += '\n';
  });

  return context;
}

/**
 * T·∫°o prompt cho RAG
 */
function createRAGPrompt(question, context) {
  return `B·∫°n l√† tr·ª£ l√Ω AI th√¥ng minh c·ªßa th∆∞ vi·ªán s√°ch. Nhi·ªám v·ª• c·ªßa b·∫°n l√† tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng d·ª±a tr√™n c√°c t√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p.

NGUY√äN T·∫ÆC QUAN TR·ªåNG:
1. CH·ªà s·ª≠ d·ª•ng th√¥ng tin t·ª´ c√°c t√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p b√™n d∆∞·ªõi
2. N·∫øu th√¥ng tin kh√¥ng c√≥ trong t√†i li·ªáu, h√£y n√≥i r√µ "T√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin n√†y trong c∆° s·ªü d·ªØ li·ªáu"
3. Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, ng·∫Øn g·ªçn, r√µ r√†ng v√† th√¢n thi·ªán
4. N·∫øu c√≥ nhi·ªÅu s√°ch li√™n quan, h√£y gi·ªõi thi·ªáu t·∫•t c·∫£
5. Lu√¥n ƒë·ªÅ c·∫≠p ƒë·∫øn ti√™u ƒë·ªÅ s√°ch v√† t√°c gi·∫£ khi ƒë∆∞a ra ƒë·ªÅ xu·∫•t

${context}

C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng: ${question}

Tr·∫£ l·ªùi:`;
}

/**
 *  Chat v·ªõi RAG - H√†m ch√≠nh
 */
export async function chat(question, options = {}) {
  try {
    const {
      topK = DEFAULT_TOP_K,
      stream = false
    } = options;

    console.log('üîç ƒêang t√¨m ki·∫øm t√†i li·ªáu li√™n quan...');

    //vector search
    const relevantBooks = await searchBooks(question, topK);

    console.log(`‚úì T√¨m th·∫•y ${relevantBooks.length} t√†i li·ªáu li√™n quan`);

    const context = buildContext(relevantBooks);
    const prompt = createRAGPrompt(question, context);

    console.log('ƒêang t·∫°o c√¢u tr·∫£ l·ªùi...');

    if (stream) {
      return {
        books: relevantBooks,
        stream: await generateStreamResponse(prompt)
      };
    } else {
      const response = await ollama.chat({
        model: CHAT_MODEL,
        messages: [{ role: 'user', content: prompt }],
        stream: false
      });

      return {
        answer: response.message.content,
        books: relevantBooks,
        question: question
      };
    }
  } catch (error) {
    console.error('Error in RAG chat:', error);
    throw error;
  }
}

/**
 * Stream response cho real-time chat
 */
async function generateStreamResponse(prompt) {
  return ollama.chat({
    model: CHAT_MODEL,
    messages: [{ role: 'user', content: prompt }],
    stream: true
  });
}

/**
 * Chat v·ªõi l·ªãch s·ª≠ h·ªôi tho·∫°i (multi-turn conversation)
 */
export async function chatWithHistory(question, conversationHistory = [], topK = DEFAULT_TOP_K) {
  try {

    //vector search
    const relevantBooks = await searchBooks(question, topK);
    const context = buildContext(relevantBooks);

    const messages = [
      {
        role: 'system',
        content: `B·∫°n l√† tr·ª£ l√Ω AI th√¥ng minh c·ªßa th∆∞ vi·ªán s√°ch. CH·ªà s·ª≠ d·ª•ng th√¥ng tin t·ª´ c√°c t√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p.\n\n${context}`
      },
      ...conversationHistory,
      { role: 'user', content: question }
    ];

    const response = await ollama.chat({
      model: CHAT_MODEL,
      messages: messages,
      stream: false
    });

    return {
      answer: response.message.content,
      books: relevantBooks,
      question: question
    };
  } catch (error) {
    console.error('Error in chat with history:', error);
    throw error;
  }
}

/**
 * ƒê·ªÅ xu·∫•t s√°ch d·ª±a tr√™n s·ªü th√≠ch
 */
export async function recommendBooks(userPreference, limit = 5) {
  try {
    //const embedding = await generateEmbedding(userPreference);
    //const books = await Book.vectorSearch(embedding, limit);

    //vector search
    const books = await searchBooks(userPreference, limit);

    return {
      recommendations: books,
      preference: userPreference
    };
  } catch (error) {
    console.error('Error recommending books:', error);
    throw error;
  }
}

/**
 *  T√≥m t·∫Øt n·ªôi dung m·ªôt cu·ªën s√°ch
 */
// export async function summarizeBook(bookId) {
//   try {
//     const book = await Book.findById(bookId);
//     if (!book) throw new Error('Kh√¥ng t√¨m th·∫•y s√°ch');

//     const prompt = `H√£y t√≥m t·∫Øt ng·∫Øn g·ªçn v·ªÅ cu·ªën s√°ch sau:

// Ti√™u ƒë·ªÅ: ${book.title}
// T√°c gi·∫£: ${book.author}
// Th·ªÉ lo·∫°i: ${book.category}
// M√¥ t·∫£: ${book.description}

// T√≥m t·∫Øt (3-5 c√¢u):`;

//     const response = await ollama.chat({
//       model: CHAT_MODEL,
//       messages: [{ role: 'user', content: prompt }],
//       stream: false
//     });

//     return {
//       book: {
//         _id: book._id,
//         title: book.title,
//         author: book.author,
//         category: book.category,
//         coverImage: book.coverImage
//       },
//       summary: response.message.content
//     };
//   } catch (error) {
//     console.error('Error summarizing book:', error);
//     throw error;
//   }
// }
